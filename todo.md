- [ ] R bindings
- [ ] Python tutorials
- [ ] generic cuda codes
- [ ] baseline CPU implementation
- [x] support pointer in device global mem
- [x] implement non-scalar kernels (Glaun√®s & Michelli)
- [x] everybody knows how to use Git !

For the 20th of Dec.:

Jean :
- [ ] Adapt cudaconv.py to make it generic
- [x] PyTorch recursive syntax
- [ ] implement non-isotropic kernels (diagonal, full symmetric tensor)
- [ ] Derivatives w.r.t. \Sigma (scalar, diagonal, full) + (fixed, tied, independent)
- [ ] Let KernelProduct choose 1D/2D, CPU/GPU
- [ ] Gaussian Mixtures
- [ ] Other Applications

Benjamin :
- [ ] 1D scheme for the generic code
- [ ] R bindings
- [ ] Check the limits of the GPU device, gridsizes
- [ ] Rules for choosing 1D/2D

Joan :
- [ ] Full on-device
- [ ] Speed benchmarks; Symbolic Diff vs. Hardcoded vs. PyTorch
- [ ] Mex files

