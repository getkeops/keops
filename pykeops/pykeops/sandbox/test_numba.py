# -*- coding: utf-8 -*-
"""numba_gauss

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1jd1fn2ZFCI_7YJT0NXdZU0mMjSJGDa73
"""

import numpy as np
from numba import cuda, njit, float32
from math import exp
from timeit_magic import timeit

@cuda.jit
def gauss_kernel(out, x, y, b):
  i = cuda.grid(1)

  # Don't continue if our index is outside the domain
  if i >= len(x):
    return

  res = 0

  for j in range(len(y)):
    res += exp(-(x[i]-y[j])**2) * b[j]

  out[i] = res

def gauss_kernel_gpu(out, x, y, b):
  cuda.synchronize()
  gauss_kernel.forall(len(x))(out, x, y, b)
  cuda.synchronize()

TPB = 192
DIMY = 2*TPB

@cuda.jit(fastmath=True, opt=True)
def fast_gauss_kernel(out, x, y, b):
  i = cuda.grid(1)
  #lx = cuda.local.array(shape=1, dtype=float32)
  syb = cuda.shared.array(shape=0, dtype=float32)
  #sb = cuda.shared.array(shape=TPB, dtype=float32)
  nblocks = cuda.gridDim.x    # blocks per grid
  res = 0
  if i<len(x):
    lx = x[i]
  for ind_block in range(nblocks):
    tj = cuda.threadIdx.x
    j = ind_block * TPB + tj
    if j<len(y):
      syb[tj*2] = y[j]
      syb[tj*2+1] = b[j]
    cuda.syncthreads()
    if i<len(x):
      for tj in range(TPB):
        j = ind_block * TPB + tj
        if j<len(y):
          res += exp(-(lx-syb[tj*2])**2) * syb[tj*2+1]
    cuda.syncthreads()
  if i<len(x):
    out[i] = res

def fast_gauss_kernel_gpu(out, x, y, b):
  threadsperblock = TPB
  blockspergrid = (len(x) + (threadsperblock - 1)) // threadsperblock
  sharedmem = DIMY * 4
  cuda.synchronize()
  fast_gauss_kernel[blockspergrid, threadsperblock, 0, sharedmem](out, x, y, b)
  cuda.synchronize()

@njit
def gauss_kernel_cpu(out, x, y, b):
  for i in range(len(x)):
    res = 0
    for j in range(len(y)):
      res += exp(-(x[i]-y[j])**2) * b[j]
    out[i] = res

def RunExample():
    N = 100000
    x = np.random.rand(N).astype(np.float32)
    y = np.random.rand(N).astype(np.float32)
    b = np.random.randn(N).astype(np.float32)
    out = np.empty(N, dtype=np.float32)

    x_gpu = cuda.to_device(x)
    y_gpu = cuda.to_device(y)
    b_gpu = cuda.to_device(b)
    out_gpu = cuda.to_device(out)

    fast_gauss_kernel_gpu(out_gpu, x_gpu, y_gpu, b_gpu)
    timeit("fast_gauss_kernel_gpu(out_gpu, x_gpu, y_gpu, b_gpu)", repeat=1, number=1, globals={**globals(),**locals()})
    out_cuda = out_gpu.copy_to_host()

    gauss_kernel_gpu(out_gpu, x_gpu, y_gpu, b_gpu)
    timeit("gauss_kernel_gpu(out_gpu, x_gpu, y_gpu, b_gpu)", repeat=1, number=1, globals={**globals(),**locals()}) 
    out_cuda = out_gpu.copy_to_host()

    if N<20000:
        timeit("gauss_kernel_cpu(out, x, y, b)", repeat=1, number=1, globals={**globals(),**locals()}) 

if __name__ == "__main__":
  RunExample()