{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fitting a generative model using standard divergences between measures\n",
    "(Author: Jean Feydy; this notebook is not finished yet, as it lacks a LibKP backend.)\n",
    "\n",
    "In this notebook, we show how to tune the parameters of a generative model to match it with an empirical distribution.\n",
    "Having observed a sample $y_1, \\dots, y_M \\in \\mathbb{R}^d$, we assume that the dataset was drawn according to an i.i.d. process of law $\\mu_{\\text{gen}} \\in \\mathbb{P}(\\mathbb{R}^d)$ which models an unknown generative process.\n",
    "We represent the **target** empirical distribution as a probability measure\n",
    "$$ \\nu = \\frac{1}{M} \\sum_j \\delta_{y_j} = \\sum_j \\nu_j \\, \\delta_{y_j}.$$\n",
    "Generically, the training dataset is thus modeled as a **sum of weighted dirac masses** which samples an unknown probability measure $\\mu_{\\text{gen}}$.\n",
    "\n",
    "**The density fitting problem.** Given this input distribution $\\nu$, we wish to extract its main features and, if possible, mimick it by generating \"plausible\" vectors $x_1, \\dots, x_N \\in \\mathbb{R^d}$; regressing $\\mu_{\\text{gen}}$ in an intelligent way. An obvious solution would be to draw samples from the finite set $\\{y_1, \\dots, y_M\\}$ with weights given by the $\\nu_j$'s...\n",
    "But this is not very satisfying. Intuitively, if the $y_j$'s were drawn according to a Gaussian distribution, we should be able to regress its mean and covariance matrix; not restrict ourselves to a finite sampling!\n",
    "Hence, **to give a relevant answer to a density estimation problem, we must introduce some prior**; assert that some probability measures (say, Gaussian laws) are much more likely to be \"the real underlying distribution $\\mu_{\\text{gen}}$\" than finite probability measures on some randomish point cloud.\n",
    "\n",
    "**An explicit cost to minimize.** Today, in line with a ``PyTorch``-friendly algorithmic paradigm, we choose to encode our prior in **the explicit structure of a generating program**.\n",
    "A deterministic application $f : \\mathbb{R}^n \\rightarrow \\mathbb{R}^d$ (i.e. a vector-valued program) defines a natural pushforward action \n",
    "\n",
    "$$ f\\# : \\mathbb{P}(\\mathbb{R}^n) \\rightarrow \\mathbb{P}(\\mathbb{R}^d) $$\n",
    "\n",
    "such that sampling $f\\#\\mu_0$ is equivalent to applying $f$ to $\\mu_0$-samples.\n",
    "Hence, if $\\mu_0$ is a (simple) reference measure in $\\mathbb{R}^n$, and if $(f_w)_{w\\in \\mathbb{R}^p}$\n",
    "is a family of deterministic functions mapping $\\mathbb{R}^n$ to $\\mathbb{R}^d$\n",
    "parametrized by vectors $w\\in \\mathbb{R}^p$, we can **look for model densities $\\mu_w$ of the form**\n",
    "\n",
    "$$ \\mu_w ~=~ f_w\\# \\mu_0  \\in \\mathbb{P}(\\mathbb{R}^d), ~~ \\text{which should be close to $\\mu_{\\text{gen}}$.}$$\n",
    "\n",
    "This way, we restrict the search space to the model measures *that can be desribed using $f_w$*.\n",
    "To formulate a well-posed problem, all we need now is **a discrepancy (data attachment) formula $\\text{d}$,\n",
    "which quantifies how far away the model distribution $f_w \\# \\mu_0$ is to the target $\\nu$**.\n",
    "Assuming we have such a convenient formula at hand,\n",
    "we can then look for the optimal vector of parameters $w\\in \\mathbb{R}^p$ that minimizes the real-valued cost\n",
    "\n",
    "$$ \\text{Cost}(w) ~=~ \\text{d}( \\mu_w, \\nu ).$$\n",
    "\n",
    "Hopefully, a gradient-descent like \n",
    "scheme can converge towards a good-enough value $w^*$, and we use $\\mu_{w^*} = f_{w^*} \\# \\mu_0$\n",
    "as an estimate for the underlying distribution $\\mu_{\\text{gen}}$.\n",
    "\n",
    "**Plan of this notebook.** The procedure described above can help us to *regress densities* by enforcing a strong generative prior... But we still have to work out the details! In the first part, we introduce the `PyTorch` syntax needed to encode a simple (polynomial) generative model for measures on $\\mathbb{R}^2$. Then, we show how to implement three standard divergences $\\text{d}(\\mu_w, \\nu)$ between sampled measures:\n",
    "- The kernel distance $\\text{d}_k$, linked to the theory of Reproducing Kernel Hilbert Spaces.\n",
    "- The log-likelihood score $\\text{d}_{\\text{ML}}$, related to the Kullback-Leibler divergence of information theory.\n",
    "- The Sinkhorn distance $\\text{d}_{\\text{OT}}$, defined through a relaxation of Optimal Transport theory.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Import the standard array-related libraries (MATLAB-like)\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "#plt.style.use('dark_background')\n",
    "from math import isnan\n",
    "import matplotlib.cm as cm\n",
    "from   matplotlib.collections  import LineCollection\n",
    "%matplotlib nbagg\n",
    "plt.ion()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Import the automatic differentiation + GPU toolbox\n",
    "import torch\n",
    "from   torch          import nn\n",
    "from   torch.nn       import Parameter\n",
    "from   torch.autograd import Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Shall we use the GPU?\n",
    "use_cuda = torch.cuda.is_available()\n",
    "dtype    = torch.cuda.FloatTensor if use_cuda else torch.FloatTensor\n",
    "NPOINTS  = 200 if use_cuda else 50# Let's keep things fast by default. Feel free to increase!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A computational building block: the kernel product\n",
    "\n",
    "Most standard discrepancies $\\text{d}$ between sampled measures can be computed using a **kernel product operator**\n",
    "\n",
    "$$ \\text{KP} :~ \\big((x_i), (y_j), (\\nu_j)\\big) \\in \\mathbb{R}^{N\\cdot d}\\times \\mathbb{R}^{M\\cdot d} \\times \\mathbb{R}^{M\\cdot 1} ~~ \\mapsto ~~ \\bigg( \\sum_j k(x_i-y_j)\\,\\nu_j \\bigg)_i \\in \\mathbb{R}^{N\\cdot 1}$$\n",
    "\n",
    "where $k:\\mathbb{R}^d \\rightarrow \\mathbb{R}$ is a convolution kernel. Mathematically, this operation is known as a **discrete convolution**:\n",
    "Indeed, if $\\nu = \\sum_j \\nu_j \\delta_{y_j}$ is a discrete measure, the convolution\n",
    "product $k\\star \\nu$ is a function defined on $\\mathbb{R}^d$ by\n",
    "\n",
    "$$\\big(k\\star\\nu \\big)(x) ~=~ \\sum_j k(x-y_j) \\,\\nu_j,$$\n",
    "\n",
    "so that computing the kernel product $\\text{KP}\\big((x_i), (y_j), (\\nu_j)\\big)$\n",
    "is equivalent to computing and sampling $k\\star \\nu$ on the point cloud $(x_i)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The Kernel Product operator can be parallelized efficiently along the \"summation lines\".\n",
    "# However, combined with a naive backpropagation scheme, it can also be prohibitevely \n",
    "# memory-intensive: one has to store full N-by-M kernel matrices on the GPU.\n",
    "# To alleviate what becomes a memory bottleneck for large samples (5,000 < N,M < 100,000), \n",
    "# one can use the efficient CUDA routines provided by the \"libkp\" library.\n",
    "#                plmlab.math.cnrs.fr/benjamin.charlier/libkp\n",
    "# \n",
    "# N.B.: For toy examples, a naive PyTorch implementation will do just fine.\n",
    "#\n",
    "backend = \"libkp\" # or \"libkp\"\n",
    "\n",
    "if backend == \"pytorch\" :\n",
    "    def kernel_product(x,y,nu, mode = \"gaussian\", s = 1.) :\n",
    "        \"\"\"\n",
    "        Computes K(x_i,y_j) @ nu_j = \\sum_j k(x_i-y_j) * nu_j\n",
    "        where k is a kernel function (say, a Gaussian) of deviation s.\n",
    "        \"\"\"\n",
    "        x_i = x.unsqueeze(1)        # Shape (N,d) -> Shape (N,1,d)\n",
    "        y_j = y.unsqueeze(0)        # Shape (M,d) -> Shape (1,M,d)\n",
    "        xmy = ((x_i-y_j)**2).sum(2) # N-by-M matrix, xmy[i,j] = |x_i-y_j|^2\n",
    "        if   mode == \"gaussian\" : K = torch.exp( - xmy / (s**2) )\n",
    "        elif mode == \"laplace\"  : K = torch.exp( - torch.sqrt(xmy + (s**2)) )\n",
    "        elif mode == \"energy\"   : K = torch.pow(   xmy + (s**2), -.25 )\n",
    "        return K @ (nu.view(-1,1))  # Matrix product between the Kernel operator and the vector nu.\n",
    "    \n",
    "elif backend == \"libkp\" :\n",
    "    import os.path\n",
    "    import sys\n",
    "    sys.path.append( '..'+os.path.sep+'..'+os.path.sep+'..'+os.path.sep+'..')\n",
    "    from libkp.cuda.generic.autodiff.python_bindings.pytorch.kernels import GaussianKernel\n",
    "    \n",
    "    def kernel_product(x,y,nu, mode = \"gaussian\", s = 1.) :\n",
    "        c = Variable(torch.Tensor([1/(s**2)])).type(dtype)\n",
    "        if mode == \"gaussian\" :\n",
    "            return GaussianKernel( c, x, y, nu.view(-1,1))\n",
    "        else :\n",
    "            raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A GMM-polynomial generative model\n",
    "\n",
    "Using `PyTorch` (or any other automatic differentiation library), defining generative models is easy: from mixture models to ODE flows, if you can define it, you can implement it. To keep things simple in this demo notebook, we define a **polynomial Gaussian Mixture Model** as follow:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def PolynomialMapping( coeffs, T ) :\n",
    "    \"\"\"\n",
    "    Given a set of coefficients   \"coeffs = [[x_0,y_0], [x_1,y_1], ..., [x_d,y_d]]\"\n",
    "    and a vector of \"time\" values \"   T   = [ t_0, t_1, ... ]                     \",\n",
    "    outputs a len(T)-by-2 torch variable X, such that\n",
    "    \n",
    "    X[i] = [ \\sum_{k=0}^d x_k*(t_i)^k  ,  \\sum_{k=0}^d y_k*(t_i)^k ]\n",
    "    \n",
    "    X can be thought of as the discretization at time steps T[:]\n",
    "    of the polynomial curve [ x(t), y(t) ] whose coefficients were provided by the user.\n",
    "    \"\"\"\n",
    "    X = Variable(torch.zeros( 2, len(T) ).type(dtype))\n",
    "    for (d, c) in enumerate(coeffs) :\n",
    "        X += c.view(2,1) * ( T.view(1,len(T)) )**d\n",
    "    return X.t().contiguous() # We need contiguity to apply libkp CUDA kernels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GenerativeModel(nn.Module) :\n",
    "    \"\"\"\n",
    "    This Model generates discrete samples in the plane R^2, according to\n",
    "    the following process:\n",
    "    - a class label 'i' is drawn according to a Bernouilli law:\n",
    "        - proba =  P  to be equal to -1\n",
    "        - proba = 1-P to be equal to +1\n",
    "    - a real number t is drawn according to a Gaussian law N(mu_i, sigma_i)\n",
    "      whose parameters depend on the class:\n",
    "        mu_{-1} = -1, mu_{+1} = +1, whereas sigma_{-1} and sigma_{+1}\n",
    "        are two independent parameters of the model.\n",
    "    - t is mapped to the 2D plane using a polynomial mapping\n",
    "      which is the same whatever the value of the class label.\n",
    "    \n",
    "    This class can therefore be seen as a differentiable implementation\n",
    "    of a Gaussian Mixture Model on the real line R, embedded in the plane\n",
    "    by a polynomial mapping [x(t),y(t)].\n",
    "    \n",
    "    \n",
    "    Practical implementation:\n",
    "    \n",
    "    Even though this notebook is not dedicated to \"Neural Networks\",\n",
    "    we use the convenient syntax introduced by nn.Module and nn.Parameter.\n",
    "    \n",
    "    Getting \"tunable\" parameters for sigma_{-1}, sigma_{+1} and the polynoms'\n",
    "    coefficients is easy: they can be understood as parameters of a differentiable\n",
    "    function      s -> t = sigma_i*s + mu_i -> [x(t), y(t)]\n",
    "    which maps samples drawn from a normal Gaussian law N(0,1)\n",
    "    to the plane.\n",
    "    \n",
    "    However, the Bernouilli parameter P \\in [0,1] is more difficult to handle:\n",
    "    it does not act *smoothly* on a simple probability law\n",
    "    (at best, you could say that it sets a threshold on a uniform law).\n",
    "    In order to get a meaningful gradient, we go for the simplest \"hack\":\n",
    "    instead of generating ~NP samples of class -1 and ~N(1-P) samples of class +1,\n",
    "    we generate N/2 samples of both classes, and *weigh* them using\n",
    "    a weights vector whose values are set to [P, ... P, (1-P), ..., (1-P)] /N.\n",
    "    \n",
    "    On top of this, as we wish to constrain P to take values in [0,1],\n",
    "    we express it as a mapped value      P = h(p)\n",
    "    where p is a real number, and h is a sigmoid function\n",
    "                    h : x -> 1 / (1+exp(-x)).\n",
    "                    \n",
    "    All in all, we generate a *measure*, encoded as a (weights, positions)\n",
    "    pair of arrays, which depends smoothly on a set of parameters\n",
    "        w = (p, sigma_{-1}, sigma_{+1}, coeffs(x), coeffs(y)).\n",
    "    \"\"\"\n",
    "    def __init__(self, p = None, sigmas = None, coeffs = None, degree = 1) :\n",
    "        \"Defines the parameters of the model, using default values if necessary.\"\n",
    "        super(GenerativeModel, self).__init__()\n",
    "        if p      is None : p      = [0]\n",
    "        if sigmas is None : sigmas = [.1,.1]\n",
    "        if coeffs is None : coeffs = [[.1]*(degree+1), [.1]*(degree+1)]\n",
    "            \n",
    "        self.p      = Parameter(torch.from_numpy(np.array(p     )).type(dtype))\n",
    "        self.sigmas = Parameter(torch.from_numpy(np.array(sigmas)).type(dtype))\n",
    "        self.coeffs = Parameter(torch.from_numpy(np.array(coeffs).T).type(dtype))\n",
    "\n",
    "    def forward(self, N) :\n",
    "        \"\"\"\n",
    "        Assuming that N is an integer, generates:\n",
    "        - a batch X_i of N points in the plane - an (N,2) array\n",
    "        - a vector of weights M_i - an (N,) vector.\n",
    "        The generated measure should be understood as being equal to\n",
    "                Mu = \\sum_i M_i*\\delta_{X_i}\n",
    "        \"\"\"\n",
    "        cut = N//2\n",
    "        \n",
    "        # Sample a Gaussian distribution, and distort it to end up on\n",
    "        # the two normal laws N(-1, sigma_0), N(1, sigma_1)\n",
    "        T = Variable(torch.normal( means = torch.zeros(N) ).type(dtype) )\n",
    "        T = torch.cat( (self.sigmas[0] * T[:cut] - 1. ,\n",
    "                        self.sigmas[1] * T[cut:] + 1. ) )\n",
    "        \n",
    "        # Map the 1D coefficients to the 2D plane through a polynomial mapping\n",
    "        X = PolynomialMapping(self.coeffs, T)\n",
    "        \n",
    "        # Compute the weights associated to our diracs :\n",
    "        # overall, mass P for the first cluster and 1-P for the second one.\n",
    "        P = 1. / (1. + torch.exp(-self.p))\n",
    "        W = torch.cat( ( (   P  / cut    ) * Variable(torch.ones(  cut).type(dtype)) ,\n",
    "                         ((1-P) / (N-cut)) * Variable(torch.ones(N-cut).type(dtype)) ) )\n",
    "        \n",
    "        return W, X\n",
    "    \n",
    "    def plot(self, axis, color = 'b') :\n",
    "        \"Displays the curve associated to the underlying polynomial mapping.\"\n",
    "        # (x(t),y(t)) for t in [-5,5]\n",
    "        t = Variable(torch.linspace(-5,5,101).type(dtype))\n",
    "        X = PolynomialMapping(self.coeffs, t)\n",
    "        X = X.data.cpu().numpy()\n",
    "        axis.plot( X[:,0], X[:,1], color+'-+', markersize = 8, linewidth=.5, zorder=-1 )\n",
    "        \n",
    "        # Puts two large dots at the \"centers\" of both sigmoids\n",
    "        t = Variable(torch.linspace(-1,1,2).type(dtype))\n",
    "        X = PolynomialMapping(self.coeffs, t)\n",
    "        X = X.data.cpu().numpy()\n",
    "        axis.scatter( X[:,0], X[:,1], 125, color, edgecolors='none' )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Our toy dataset \n",
    "\n",
    "Now that our model $f_w: \\mathbb{R}\\rightarrow \\mathbb{R}^2$ is defined - with $w = \\big(p, \\sigma_{-1}, \\sigma_{+1}, \\text{coeffs}(x), \\text{coeffs}(y)\\big)$ - we specify our \"Ground Truth\" model $\\mu_{\\text{gen}}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a model: two unbalanced classes mapped through a polynomial curve of degree = 3\n",
    "GroundTruth = GenerativeModel(p = [1.], sigmas = [.2,.4], coeffs = [[-.2,1,+.2,-.3], [-1,-.5,.9,.2]])\n",
    "# print(GroundTruth.p, GroundTruth.sigmas, GroundTruth.coeffs) # Parameters of the model are easily accessible"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we sample it and add some random Gaussian noise to produce our target, the *observed empirical measure*\n",
    "\n",
    "$$\\nu ~=~ \\sum_j \\nu_j\\,\\delta_{y_j}.$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample our \"ground truth\" distribution, and add some Gaussian noise\n",
    "(Nu_j, Y_j) = GroundTruth(NPOINTS)\n",
    "Y_j = Y_j + .05 * Variable(torch.normal( means = torch.zeros(Y_j.size()) ).type(dtype)) \n",
    "Nu_j = Nu_j.detach()\n",
    "Y_j  = Y_j.detach()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To display our point cloud, we first have to turn our\n",
    "# Pytorch variables (possibly GPU-enabled) into numpy arrays\n",
    "nu_j = Nu_j.data.cpu().numpy() ; y_j = Y_j.data.cpu().numpy()\n",
    "\n",
    "# We can then use standard matplotlib routines:\n",
    "fig = plt.figure(figsize=(8,8))\n",
    "ax  = plt.subplot(1,1,1)\n",
    "ax.scatter( y_j[:,0], y_j[:,1], 6400*nu_j, 'g', edgecolors='none' )\n",
    "plt.axis('equal')   ; plt.tight_layout()\n",
    "ax.set_xlim([-2,2]) ; ax.set_ylim([-2,2])\n",
    "fig.canvas.draw()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The default model (starting point) v. our target dataset\n",
    "\n",
    "We are about to use gradient descent to minimize\n",
    "\n",
    "$$ \\text{Cost}(w) ~=~ \\text{d}( \\mu_w, \\nu ) ~=~ \\text{d}( f_w \\# \\mu_0, \\nu),$$\n",
    "\n",
    "with some user-defined formula $\\text{d}$. As a starting point, we should thus use default values for $w$, $f_w$ and $\\mu_w$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# A \"default\" polynomial model of degree 3.\n",
    "MyModel = GenerativeModel(degree = 2)\n",
    "(Mu_i, X_i) = MyModel(NPOINTS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's display it next to our \"Ground Truth\"!\n",
    "nu_j = Nu_j.data.cpu().numpy() ; y_j = Y_j.data.cpu().numpy()\n",
    "mu_i = Mu_i.data.cpu().numpy() ; x_i = X_i.data.cpu().numpy()\n",
    "\n",
    "fig = plt.figure(figsize=(8,8))\n",
    "ax  = plt.subplot(1,1,1)\n",
    "GroundTruth.plot(ax, 'g')\n",
    "MyModel.plot(    ax, 'm')\n",
    "ax.scatter( y_j[:,0], y_j[:,1], 6400*nu_j, 'b', edgecolors='none' )\n",
    "ax.scatter( x_i[:,0], x_i[:,1], 6400*mu_i, 'r', edgecolors='none' )\n",
    "\n",
    "plt.axis('equal')   ; plt.tight_layout()\n",
    "ax.set_xlim([-2,2]) ; ax.set_ylim([-2,2])\n",
    "fig.canvas.draw()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get prepared for model optimization\n",
    "\n",
    "To choose a useful model, we'll rely on visual intuition. Hence, we need to implement some visualization routines! For the sake of completeness, this tutorial provides the full code needed to run  the demo: If you're only interested in the mathematical aspects, feel free to skip this cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def plot_situation(fig, ax, ground_truth,     mymodel, \n",
    "                            training_measure, generated_measure,\n",
    "                            info_type,        info             ) :\n",
    "    \"\"\"\n",
    "    During the model-fitting loop, plots the evolution of the\n",
    "    trained model next to our \"ground truth\".\n",
    "    \"\"\"\n",
    "    # Turn PyTorch variables into numpy arrays --------------------------\n",
    "    nu_j =  training_measure[0].data.cpu().numpy()\n",
    "    y_j  =  training_measure[1].data.cpu().numpy()\n",
    "    mu_i = generated_measure[0].data.cpu().numpy()\n",
    "    x_i  = generated_measure[1].data.cpu().numpy()\n",
    "    \n",
    "    # Remove the colorbar if any, clear the axis ------------------------\n",
    "    if len(ax.images) > 0 :\n",
    "        cbars = ax.images[0].colorbar\n",
    "        if cbars is not None : \n",
    "            cbars.remove()\n",
    "    ax.clear()\n",
    "    \n",
    "    # Plot the \"info\" image/transport-plan, if any ----------------------\n",
    "    if info is not None :\n",
    "        info  = info.data.cpu().numpy()\n",
    "        if info_type == \"heatmap\" :     # The info is a signal defined on the plane\n",
    "            # We display it as a background image\n",
    "            scale = np.amax(np.abs(info)) * 1.5\n",
    "            cax   = ax.imshow(info, interpolation='bilinear', origin='lower', \n",
    "                      vmin = -scale, vmax = scale, cmap=cm.bwr, \n",
    "                      extent=(-2,2, -2, 2), zorder=-2)\n",
    "            cbar  = fig.colorbar(cax)\n",
    "        if info_type == \"log\" :     # The info is the log of a signal defined on the plane\n",
    "            # We display it as a background image\n",
    "            scale = np.amax(np.abs(info)) \n",
    "            cax   = ax.imshow(info, interpolation='bilinear', origin='lower', \n",
    "                      vmin = -3*scale, vmax = scale, cmap=cm.bwr, \n",
    "                      extent=(-2,2, -2, 2), zorder=-2)\n",
    "            cbar  = fig.colorbar(cax)\n",
    "        elif info_type == \"transport\" : # The info is a transport plan between the two measures\n",
    "            # We display it as a \"spider's web\" linking the training measure to the generated one.\n",
    "            segs = []\n",
    "            # (the code below is here to produce fancy plots, no need to worry about it)\n",
    "            for (xi, mui, gi) in zip(x_i, mu_i, info) :\n",
    "                if False :\n",
    "                    segs += [ [xi, y_j[np.argmax(gi)] ] ]\n",
    "                else :\n",
    "                    gi = gi / mui # gi[j] = fraction of the mass from \"a\" which goes to targetpoints[j]\n",
    "                    for (yj, gij) in zip(y_j, gi) :\n",
    "                        mass_per_line = 0.02\n",
    "                        if gij >= mass_per_line :\n",
    "                            nlines = np.floor(gij / mass_per_line)\n",
    "                            ts     = np.linspace(-.005*(nlines-1), .005*(nlines-1), nlines)\n",
    "                            for t in ts :\n",
    "                                b = yj + t*np.array([ yj[1]-xi[1], -yj[0]+xi[0]])\n",
    "                                segs += [[xi, b]]\n",
    "                            \n",
    "            line_segments = LineCollection(np.array(segs), linewidths=(.3,), \n",
    "                                           colors=[(.6,.8,1.)]*len(segs), linestyle='solid', zorder=-1)\n",
    "            ax.add_collection(line_segments)\n",
    "    \n",
    "    # Plot the model \"embeddings\", and the associated point clouds ------\n",
    "    ground_truth.plot(ax, 'g')\n",
    "    mymodel.plot(ax, 'm')\n",
    "    ax.scatter( y_j[:,0], y_j[:,1], 6400*nu_j, 'b', edgecolors='none' )\n",
    "    ax.scatter( x_i[:,0], x_i[:,1], 6400*mu_i, 'r', edgecolors='none' )\n",
    "    \n",
    "    # Ready to plot ! ---------------------------------------------------\n",
    "    plt.axis('equal')  ; plt.tight_layout() ; ax.set_xlim([-2,2]) ; ax.set_ylim([-2,2])\n",
    "    fig.canvas.draw()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The optimization routine, though, is pretty important:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def FitModel(Model, Fidelity, EmpiricMeasure, name = \"model\", info_type = \"heatmap\", \n",
    "             lr=.1, NITS=500, eps=1e-2, **kwargs) :\n",
    "    \"\"\"\n",
    "    Given an EmpiricMeasure (observed), and a Fidelity \"error assessment formula\"\n",
    "    betweens measures, this routines fits an arbitrary generative Model\n",
    "    to make it generate sample distributions close to the empirical one.\n",
    "    \"\"\"\n",
    "    # We'll minimize \"Fidelity( SampledModelDistribution, EmpiricMeasure )\"\n",
    "    # with respect to the model's parameters using a standard gradient-like\n",
    "    # descent scheme. As we do not perform any kind of line search, \n",
    "    # this algorithm may diverge if the learning rate is too large !\n",
    "    # For robust optimization routines, you may consider using\n",
    "    # the scipy.optimize API with a \"parameters <-> float64 vector\" wrapper.\n",
    "    optimizer = torch.optim.Adam(Model.parameters(), lr=lr, eps=eps)\n",
    "    \n",
    "    # We'll plot results on-the-fly, and save the list of costs across iterations\n",
    "    fig = plt.figure(figsize=(10,8)) ; ax  = plt.subplot(1,1,1)\n",
    "    costs = [] ; N = len(EmpiricMeasure[0]) ; NLOG = 50\n",
    "    \n",
    "    FitModel.nit = -1 ; FitModel.breakloop = False\n",
    "    def closure():\n",
    "        \"\"\"\n",
    "        Encapsulates a problem + display iteration into a single callable statement.\n",
    "        This wrapper is needed if you choose to use LBFGS-like algorithms, which\n",
    "        (should) implement a careful line search along the gradient's direction.\n",
    "        \"\"\"\n",
    "        FitModel.nit += 1 ; i = FitModel.nit\n",
    "        # Minimization loop --------------------------------------------------------------------\n",
    "        optimizer.zero_grad()                      # Reset the gradients (PyTorch syntax...).\n",
    "        GeneratedMeasure = Model(N)                # Draw a random sample from our model.\n",
    "        Cost, info = Fidelity( GeneratedMeasure, EmpiricMeasure,  # Compute the discrepancy\n",
    "                               info = (i%NLOG==0), **kwargs )     # wrt. the empirical distrib.\n",
    "        costs.append(Cost.data.cpu().numpy()[0])   # Store the \"cost\" for plotting.\n",
    "        Cost.backward(retain_graph=True)                            # Backpropagate to compute the gradient.\n",
    "        \n",
    "        if i % NLOG == 0: # Display the current model ------------------------------------------\n",
    "            print(\"Iteration \",i,\"Cost = \", Cost.data.cpu().numpy()[0])\n",
    "            plot_situation(fig, ax, GroundTruth,    Model, \n",
    "                                    EmpiricMeasure, GeneratedMeasure,\n",
    "                                    info_type, info                   )\n",
    "            fig.savefig('output/'+name+'_'+str(i)+'.png', dpi=fig.dpi) # -----------------------\n",
    "        \n",
    "        if isnan(costs[-1]) : FitModel.breakloop = True\n",
    "        return Cost\n",
    "    \n",
    "    for i in range(NITS+1) :           # Fixed number of iterations\n",
    "        optimizer.step(closure)     # \"Gradient descent\" step.\n",
    "        if FitModel.breakloop : break\n",
    "            \n",
    "        \n",
    "    # Once the minimization is over, display the cost evolution --------------------------------\n",
    "    fig = plt.figure(figsize=(8,8)) ;  ax  = plt.subplot(1,1,1)\n",
    "    ax.plot(np.array(costs))\n",
    "    fig.canvas.draw()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using a kernel norm\n",
    "\n",
    "**Total Variation: a first dual norm.** Now, it's time to choose the last piece of the puzzle: the data attachment formula\n",
    "$d(\\mu_w, \\nu)$ whose gradient (wrt. $w$) is going to drive our optimization routine.\n",
    "Given two measures $\\mu$ and $\\nu$ on $\\mathbb{R}^d$, one of the simplest distance that can be defined\n",
    "is the Total Variation\n",
    "\n",
    "$$\\text{d}_{\\text{TV}}(\\mu,\\nu) ~=~ \\|\\mu-\\nu\\|_{\\infty}^{\\star} ~=~ \\sup_{\\|f\\|_{\\infty} \\leqslant 1} \\int f \\text{d}\\mu - \\int f \\text{d}\\nu,$$\n",
    "\n",
    "using the dual norm on $L^{\\infty}(\\mathbb{R}^d, \\mathbb{R})$.\n",
    "Unfortunately, this formula is not suited *at all* to sampled, discrete probability measures with non-overlapping support:\n",
    "If $\\mu = \\sum_i \\mu_i\\,\\delta_{x_i}$ and $\\nu = \\sum_j \\nu_j\\,\\delta_{y_j}$ with $\\{x_i, \\dots\\}\\cap\\{y_j,\\dots\\} = \\emptyset$, one can simply choose a function $f$ such that \n",
    "\n",
    "$$\\forall \\, i,~ f(x_i) ~=~+1 ~~~ \\text{and} ~~~ \\forall \\, j, ~f(y_j) ~=~-1$$\n",
    "\n",
    "to show that\n",
    "\n",
    "$$\\text{d}_{\\text{TV}}(\\mu, \\nu) ~=~ |\\mu| + |\\nu| ~=~ 2 ~~~~ \\text{as soon as $\\text{supp}(\\mu)$ and $\\text{supp}(\\nu)$ do not overlap.}$$\n",
    "\n",
    "The gradient of the Total Variation distance between two sampled measures is thus completely uninformative, being zero for almost all configurations.\n",
    "\n",
    "**Smoothing measures to create overlap.**\n",
    "How can we fix this problem? An idea would be to choose a **blurring function $g$**, and compare\n",
    "the blurred functions $g\\star \\mu$ and $g\\star \\nu$ by using, say, an $L^2$ norm:\n",
    "\n",
    "$$\\text{d}(\\mu, \\nu) ~=~ \\| g\\star(\\mu-\\nu)\\|_2^2 ~=~ \\langle g\\star(\\mu-\\nu), g\\star(\\mu-\\nu)\\rangle_2.$$\n",
    "\n",
    "But then, if we define $k = \\tilde{g}\\star g$, where $\\tilde{g} = g \\circ (x\\mapsto -x)$ is the\n",
    "mirrored blurring function, one gets\n",
    "\n",
    "$$\\text{d}_k(\\mu,\\nu) ~=~ \\langle g\\star(\\mu-\\nu), g\\star(\\mu-\\nu)\\rangle_2\n",
    "~=~ \\langle \\mu-\\nu, k\\star(\\mu-\\nu)\\rangle ~=~ \\|\\mu-\\nu\\|_k^2.$$\n",
    "\n",
    "Assuming a few properties on $k$ (detailed below), $\\text{d}_k$ is the quadratic norm associated with the $k$-scalar product between measures:\n",
    "\n",
    "$$\\langle \\mu, \\nu \\rangle_k ~=~ \\langle \\mu, k\\star \\nu\\rangle.$$\n",
    "\n",
    "More specifically,\n",
    "\n",
    "$$\\bigg\\langle \\sum_i \\mu_i \\, \\delta_{x_i} , \\sum_j \\nu_j\\,\\delta_{y_j} \\bigg\\rangle_k\n",
    "~=~\\bigg\\langle \\sum_i \\mu_i \\, \\delta_{x_i} , \\sum_j \\nu_j\\,\\big(k\\star\\delta_{y_j}\\big) \\bigg\\rangle\n",
    "~=~\\bigg\\langle \\sum_i \\mu_i \\, \\delta_{x_i} , \\sum_j \\nu_j\\,k(\\,\\cdot\\,- y_j) \\bigg\\rangle\n",
    "~=~ \\sum_{i,j} k(x_i-y_j) \\, \\mu_i \\nu_j.$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def kernel_scalar_product(Mu, Nu, mode = \"gaussian\", s = 1.) :\n",
    "    \"\"\"\n",
    "    Takes as input two measures Mu and Nu - encoded as (weights,points) pairs:\n",
    "    \n",
    "    Mu = \\sum_i mu_i*\\delta_{x_i}  ; Nu = \\sum_j nu_j*\\delta_{y_j}\n",
    "    \n",
    "    as well as parameters for a kernel function.\n",
    "    \n",
    "    Computes the kernel scalar product\n",
    "    <Mu,Nu>_k = < Mu, k \\star Nu >                    (convolution product)\n",
    "              = \\sum_{i,j} k(x_i-y_j) * mu_i * nu_j\n",
    "    \"\"\"\n",
    "    (mu, x) = Mu ; (nu, y) = Nu\n",
    "    k_nu = kernel_product(x,y,nu,mode,s)\n",
    "    return torch.dot( mu.view(-1), k_nu.view(-1)) # PyTorch syntax for the L2 scalar product..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Having defined the scalar product, we then simply develop by bilinearity:\n",
    "\n",
    "$$\\|\\mu-\\nu\\|_k^2 ~=~ \\langle \\mu,\\mu \\rangle_k \\, -\\, 2\\langle \\mu,\\nu \\rangle_k \\,+\\,\\langle \\nu,\\nu \\rangle_k.$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def kernel_distance(Mu, Nu, info = False, mode = \"gaussian\", s = 1.) :\n",
    "    \"\"\"\n",
    "    Hilbertian kernel (squared) distance between measures Mu and Nu,\n",
    "    computed using the fact that\n",
    "    \n",
    "    |Mu-Nu|^2_k  =  <Mu,Mu>_k - 2 <Mu,Nu>_k + <Nu,Nu>_k\n",
    "    \n",
    "    If \"info\" is required, we output the values of\n",
    "         k \\star (Mu-Nu)  sampled on a uniform grid,\n",
    "    to be plotted later.\n",
    "    \n",
    "    Strictly speaking, it would make more sense to display\n",
    "         g \\star (Mu-Nu)     where     g \\star g = k\n",
    "    as we would then have\n",
    "          |Mu-Nu|^2_k  =  |g \\star (Mu-Nu)|^2_{L^2}.\n",
    "        \n",
    "    But this is easy only for Gaussians...\n",
    "    \"\"\"\n",
    "    D2 =   (   kernel_scalar_product(Mu,Mu,mode,s) \\\n",
    "           +   kernel_scalar_product(Nu,Nu,mode,s) \\\n",
    "           - 2*kernel_scalar_product(Mu,Nu,mode,s) )\n",
    "    \n",
    "    kernel_heatmap = None\n",
    "    if info :\n",
    "        # Create a uniform grid on the [-2,+2]x[-2,+2] square:\n",
    "        res    = 100 ; ticks = np.linspace( -2, 2, res + 1)[:-1] + 1/(2*res) \n",
    "        X,Y    = np.meshgrid( ticks, ticks )\n",
    "        points = Variable(torch.from_numpy(np.vstack( (X.ravel(), Y.ravel()) ).T).type(dtype), requires_grad=False)\n",
    "        \n",
    "        # Sample \"k \\star (Mu-Nu)\" on this grid:\n",
    "        kernel_heatmap   = kernel_product(points, Mu[1], Mu[0], mode, s) \\\n",
    "                         - kernel_product(points, Nu[1], Nu[0], mode, s)\n",
    "        kernel_heatmap   = kernel_heatmap.view(res,res) # reshape as a \"background\" image\n",
    "    return D2, kernel_heatmap"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This formula looks good: points **interact** with each other as soon as $k(x_i,y_j)$ is non-negligible.\n",
    "But if we want to get a genuine norm between measures, what hypothesis should we make on $k$?\n",
    "\n",
    "This question was studied by mathematicians from the first half of the 20th century who developed the theory of Reproducing Kernel Hilbert Spaces - RKHS.\n",
    "In our specific translation-invariant case (in which we \"hardcode\" convolutions), the results can be summed up as follow:\n",
    "- Principled kernel norms are the ones associated to **kernel functions $k$** whose Fourier transform is *real-valued* and *positive* - think, Gaussian kernels:\n",
    "\n",
    "$$\\forall\\, \\omega \\in \\mathbb{R}^d, ~ \\widehat{k}(\\omega) > 0.$$\n",
    "  \n",
    "\n",
    "- For any such kernel function, there exists a unique blurring kernel function $g$ such that $g\\star g = k$: Simply choose\n",
    "\n",
    "$$\\widehat{g}(\\omega) ~=~ \\sqrt{ \\widehat{k}(\\omega)}.$$\n",
    "\n",
    "- These kernels define a **Hilbert norm** on a subset of $L^2(\\mathbb{R}^d)$:\n",
    "\n",
    "$$\\|f\\|_V^2 ~=~ \\int_{\\omega \\in \\mathbb{R}^d} \\frac{|\\widehat{f}(\\omega)|^2}{\\widehat{k}(\\omega)} \\,\\text{d}\\omega ~=~ \\langle k^{(-1)} \\star f\\,,\\, f\\rangle$$\n",
    "  where $k^{(-1)}$ is the deconvolution kernel associated to $k$.\n",
    "  If we define \n",
    "  \n",
    "  $$V ~=~ \\big\\{ f\\in L^2(\\mathbb{R}^d), ~\\|f\\|_V < \\infty \\big\\}, $$\n",
    "  \n",
    "  then $(V, \\|\\cdot\\|_V)$ is a Hilbert space of functions endowed with the scalar product\n",
    "  \n",
    "  $$ \\langle f\\,,\\, g\\rangle_V ~=~ \\int_{\\omega \\in \\mathbb{R}^d} \\frac{\\overline{\\widehat{f}(\\omega)} \\,\\widehat{g}(\\omega)}{\\widehat{k}(\\omega)} \\,\\text{d}\\omega ~=~ \\langle k^{(-1)} \\star f\\,,\\, g\\rangle. $$\n",
    "  \n",
    "- **We focus on kernel functions such that for all points $x\\in\\mathbb{R}^d$, the evaluation at point $x$ is a continuous linear form on $V$.** That is,\n",
    "\n",
    "$$ \\delta_x : f\\in (V, \\|\\cdot\\|_V) \\mapsto f(x) \\in (\\mathbb{R}, |\\cdot|)$$\n",
    "\n",
    "  is well-defined and continuous. A sufficient condition for this is to ask that $\\widehat{k} \\in L^1(\\mathbb{R}^d)$ and continuous. Then, we show that the Riesz theorem identifies $\\delta_x$ with the continuous function\n",
    "$k\\star \\delta_x : y \\mapsto k(y-x)$:\n",
    "\n",
    "$$ \\forall\\, f\\in V,~~  f(x)~=~\\langle \\delta_x\\,,\\, f\\rangle ~=~ \\langle k\\star\\delta_x\\,,\\, f\\rangle_V.$$\n",
    "\n",
    "- **Finite sampled measures can thus be identified with linear forms on $V$. The $k$-norm is nothing but the dual norm of $\\|\\cdot\\|_V$:**\n",
    "\n",
    "$$\\forall\\, \\mu\\in V^{\\star}, ~\\|\\mu\\|_k ~=~ \\langle \\mu\\,,\\, k\\star \\mu \\rangle ~=~ \\sup_{\\|f\\|_V \\leqslant 1} \\langle \\mu\\,,\\, f\\rangle.$$\n",
    "\n",
    "All-in-all, **just like the TV distance, the kernel distance can be seen as the dual of a norm on a space of functions**. Whereas TV was associated to the infinity norm $\\|\\cdot\\|_{\\infty}$ on $L^{\\infty}(\\mathbb{R}^d)$, the kernel formulas are linked to Sobolev-like norms $\\|\\cdot\\|_V$ on spaces of $k$-smooth functions, denoted by the letter $V$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**In practice, to match sampled measures,** we need to choose kernel functions with:\n",
    "- A null derivative at zero, and a large enough \"blurring radius\" to prevent overfit on the precise sampled locations of diracs.\n",
    "- A heavy tail, to prevent isolated parts of $\\mu_w$ and $\\nu$ from being \"forgotten\" by the gradient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "MyModel = GenerativeModel(degree = 2)\n",
    "\n",
    "# Please play around with several kernel radii and profiles!\n",
    "FitModel(MyModel, kernel_distance, (Nu_j, Y_j), NITS=1000, mode = \"gaussian\", s=.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using a Maximum-likelihood estimator\n",
    "\n",
    "In the previous section, we've seen how to compare measures by seeing them as linear forms on a Sobolev-like space of functions. An other idea would be to **see the measure $\\nu$ as the realization of an i.i.d. sampling according to the measure $\\mu$, with likelihood**\n",
    "\n",
    "$$\\text{Likelihood}_\\mu (\\nu) ~=~ \\text{Likelihood}_\\mu \\bigg(\\sum_j \\nu_j \\delta_{y_j}\\bigg)\n",
    "~=~ \\prod_j \\text{Likelihood}_\\mu(y_j)^{\\nu_j}.$$\n",
    "\n",
    "But which value could we attribute to the \"likelihood of drawing $y$, given the measure $\\mu$\"?\n",
    "Since $\\mu$ is discrete, supported by the $x_i$'s, interpreting it as a density wouldn't be practical at all...\n",
    "Thankfully, there's a simple solution: we could **convolve** $\\mu$ with a simple density function $k>0$ of mass 1 - say, a Gaussian - and thus end up with a probability measure $k\\star \\mu$ which is absolutely continous wrt. the Lebesgue measure, with density\n",
    "\n",
    "$$\\text{Likelihood}_{k\\star\\mu}(y) ~=~ \\sum_i k(y-x_i) \\,\\mu_i > 0 ~~ \\text{ for all $y\\in\\mathbb{R}^d$}.$$\n",
    "\n",
    "From a probabilistic point of view, using this density function as a \"model\" is equivalent to assuming that the random variable $y$ is generated as a sum $x + w$, where $x$ and $w$ are two independent variables of laws equal to $\\mu$ and $k\\cdot\\text{Lebesgue($\\mathbb{R}^d$)}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given $\\mu$, $\\nu$ and a symmetric kernel function $k$, we can thus choose to **maximize the likelihood**\n",
    "\n",
    "$$\\text{Likelihood}_{k\\star\\mu} (\\nu) ~=~ \\prod_j \\big( \\sum_i k(x_i-y_j) \\,\\mu_i \\big)^{\\nu_j},$$\n",
    "\n",
    "i.e. to **minimize the negative log-likelihood**\n",
    "\n",
    "$$\\text{d}_{\\text{ML},k}(\\mu,\\nu) ~=~ - \\sum_j \\log\\big( \\sum_i k(x_i-y_j) \\,\\mu_i \\big)\\, \\nu_j.$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Information theoretic interpretation.** Before going any further, we wish to stress the link between maximum-likelihood estimators and the Kullback-Leibler divergence. In fact, **if we assume that a measure $\\nu_{\\text{gen}}$ is absolutely continuous wrt. the Lebesgue measure $\\lambda$,** then\n",
    "\n",
    "$$\\text{KL}(\\nu_{\\text{gen}}\\,|\\, \\mu) \n",
    "~=~ \\int \\log \\bigg( \\frac{\\text{d} \\nu_{\\text{gen}}}{\\text{d}\\mu} \\bigg) \\,\\text{d}\\nu_{\\text{gen}}\n",
    "~=~ \\int \\log \\bigg( \\frac{\\text{d} \\nu_{\\text{gen}} / \\text{d}\\lambda }{\\text{d}\\mu/ \\text{d}\\lambda} \\bigg) \\,\\text{d}\\nu_{\\text{gen}}\n",
    "~=~ \\int \\log \\bigg( \\frac{\\text{d} \\nu_{\\text{gen}}}{\\text{d}\\lambda} \\bigg) \\,\\text{d}\\nu_{\\text{gen}}\n",
    "~-~ \\int \\log \\bigg( \\frac{\\text{d} \\mu}{\\text{d}\\lambda} \\bigg) \\,\\text{d}\\nu_{\\text{gen}}$$\n",
    "\n",
    "$$\\text{i.e.}~~ \\text{KL}(\\nu_{\\text{gen}}\\,|\\, \\mu) \n",
    "~=~ \\text{KL}(\\nu_{\\text{gen}}\\,|\\, \\lambda) \n",
    "~-~ \\int \\log \\bigg( \\frac{\\text{d} \\mu}{\\text{d}\\lambda} \\bigg) \\,\\text{d}\\nu_{\\text{gen}}\n",
    "~~~~ \\text{so that}~~ \n",
    "~- \\int \\log \\bigg( \\frac{\\text{d} \\mu}{\\text{d}\\lambda} \\bigg) \\,\\text{d}\\nu_{\\text{gen}}\n",
    "~=~ \\text{KL}(\\nu_{\\text{gen}}\\,|\\, \\mu)  - \\text{KL}(\\nu_{\\text{gen}}\\,|\\, \\lambda) .$$\n",
    "\n",
    "Hence, as the sampled measure $\\nu$ weakly converges towards a measure $\\nu_{\\text{gen}}$,\n",
    "\n",
    "$$d_{\\text{ML},k}(\\mu,\\nu) ~\\longrightarrow~ \\text{KL}(\\nu_{\\text{gen}}\\,|\\,k\\star \\mu) ~-~ \\text{KL}(\\nu_{\\text{gen}}\\,|\\, \\lambda). $$\n",
    "\n",
    "As a function of $\\mu$, this formula is minimized if and only if $~~k\\star \\mu = \\nu_{\\text{gen}}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Practical implementation.** As noted by the careful reader, the maximum-likelihood cost $\\text{d}_{\\text{ML},k}(\\mu,\\nu)$ can be computed as the scalar product between the vector of weights $(\\nu_j)$ and the pointwise logarithm of the **Kernel Product** $\\text{KP}\\big( (y_j), (x_i), (\\mu_i) \\big)$ - up to a negative sign.\n",
    "So, is using our `kernel_product` routine a sensible thing to do? **No, it isn't.**\n",
    "\n",
    "Indeed, if a point $y_j$ is far away from the support $\\{x_i, \\dots\\}$ of the measure $\\mu$, $\\sum_i k(x_i-y_j)\\,\\mu_i$ can be prohibitively small. Just remember how fast a Gaussian kernel decreases to zero!\n",
    "If this sum's order of magnitude is close to the floating point precision (for `float32` encoding, around $10^{-7}\\simeq e^{-4^2}$), applying to it a logarithmic function is just asking for trouble.\n",
    "\n",
    "**Additive v. Multiplicative formulas.** In the previous section, we defined the **kernel distance** $\\text{d}_k$ and never encountered any accuracy problem. This is because, as far as **sums** are concerned, small \"kernel's tail\" values can be safely discarded - providing a reasonable balance in the weights' distribution. However, when using maximum likelihood estimation, all the values are **multiplicated with each other**: the smaller ones cannot be \"neglected\" anymore, as they very much determine the magnitude of the whole product. In the log-domain, near-zero values of the density $\\big(k\\star \\mu\\big)(y_j)$ have a large influence on the final result!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Numerical stabilization.** We now understand the importance of **magnitude-independent schemes** as far as multiplicative formulas are concerned. Programs which do not spiral out of control when applied to values of the order of $10^{-100}$. How do we achieve such robustness? For arbitrary expressions, the only solution may be to increase the memory footprint of floating-point numbers...\n",
    "\n",
    "But in this specific \"Kernel Product\" case, a simple trick will do wonders: using a **robust log-sum-exp expression**.\n",
    "Let's write\n",
    "\n",
    "$$U_i ~=~Â \\log(\\mu_i), ~~~~~~~ C_{i,j}~=~ \\log\\big( k(x_i-y_j)\\big) ~~~ \\text{(given as a stable explicit formula)}.$$\n",
    "\n",
    "Then, the log-term in the ML distance can be written as\n",
    "\n",
    "$$\\log\\big(k\\star\\mu\\big)(y_j)\n",
    "~=~ \\log\\bigg( \\sum_i k(x_i-y_j) \\,\\mu_i \\bigg)\n",
    "~=~ \\log\\bigg( \\sum_i \\exp \\big( C_{i,j} + U_i \\big) \\bigg).$$\n",
    "\n",
    "This expression lets us see that **the order of magnitude of $\\big(k\\star\\mu\\big)(y_j)$ can be factored out easily**. Simply compute\n",
    "\n",
    "$$M_j~=~ \\max_i C_{i,j} + U_i, ~~~~~ \\text{and remark that} ~~~~~\n",
    "\\log\\big(k\\star\\mu\\big)(y_j) \n",
    "~=~ M_j \\,+\\, \\log\\bigg( \\sum_i \\exp \\big( C_{i,j} + U_i - M_j \\big) \\bigg).$$\n",
    "\n",
    "As the major exponent has been pulled out of the sum, we have effectively solved our accuracy problems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "if backend == \"pytorch\" :\n",
    "    def log_sum_exp(mat, dim):\n",
    "        \"\"\"\n",
    "        Computes the log-sum-exp of a matrix with a numerically stable scheme, \n",
    "        in the user-defined summation dimension: exp is never applied\n",
    "        to a number >= 0, and in each summation row, there is at least\n",
    "        one \"exp(0)\" to stabilize the sum.\n",
    "        \n",
    "        For instance, if dim = 1 and mat is a 2d array, we output\n",
    "                    log( sum_j exp( mat[i,j] )) \n",
    "        by factoring out the row-wise maximas.\n",
    "        \"\"\"\n",
    "        max_rc = torch.max(mat, dim)[0]\n",
    "        return max_rc + torch.log(torch.sum(torch.exp(mat - max_rc.unsqueeze(dim)), dim))\n",
    "    \n",
    "    def kernel_product_log(x,y,nu_log, mode = \"gaussian\", s = 1.) :\n",
    "        \"\"\"\n",
    "        Computes log( K(x_i,y_j) @ nu_j) = log( \\sum_j k(x_i-y_j) * nu_j) in the log domain,\n",
    "        where k is a kernel function (say, a Gaussian) of deviation s.\n",
    "        \"\"\"\n",
    "        x_i = x.unsqueeze(1)        # Shape (N,d) -> Shape (N,1,d)\n",
    "        y_j = y.unsqueeze(0)        # Shape (M,d) -> Shape (1,M,d)\n",
    "        xmy = ((x_i-y_j)**2).sum(2) # N-by-M matrix, xmy[i,j] = |x_i-y_j|^2\n",
    "        if   mode == \"gaussian\"     : C =  - xmy / (s**2) \n",
    "        elif mode == \"laplace\"      : C =  - torch.sqrt(xmy + (s**2)) \n",
    "        elif mode == \"energy\"       : C =  -.25 * torch.log( xmy + (s**2) )\n",
    "        elif mode == \"exponential\"  : C =  - torch.sqrt(xmy) / s \n",
    "        return log_sum_exp( C + nu_log.view(1,-1) , 1 ).view(-1,1) # Matrix product between the Kernel operator and the vector nu.\n",
    "    \n",
    "elif backend == \"libkp\" :\n",
    "    def kernel_product_log(x,y,nu_log, mode = \"gaussian\", s = 1.) :\n",
    "        c = Variable(torch.Tensor([1/(s**2)])).type(dtype)\n",
    "        if mode == \"gaussian\" :\n",
    "            return GaussianKernel( c, x, y, nu_log.view(-1,1), mode = \"log\")\n",
    "        else :\n",
    "            raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def kernel_neglog_likelihood(Mu, Nu, info = False, mode = \"gaussian\", s = 1.) :\n",
    "    \"\"\"\n",
    "    Returns the negative log likelihood of the sampled measure Nu,\n",
    "    assuming an i.i.d. sampling from the density Mu convolved with\n",
    "    a kernel specified by mode and s.\n",
    "    \n",
    "    The summation is done in the log-domain, for increased numerical stability.\n",
    "    \n",
    "    N.B.: for computational efficiency, kernel computations are \"raw\",\n",
    "          not normalized to have unit mass on R^d.\n",
    "          Hence, this log-likelihood can not be rigorously interpreted\n",
    "          as the log of a density function of total mass 1.\n",
    "          This can be fixed by adding the correct normalization constant\n",
    "          in front of the log-result, which is useful if one wants\n",
    "          to optimize on the kernel's parameters.\n",
    "    \n",
    "    If 'info' is True, kernel_heatmap is a background image, displaying\n",
    "    the real-valued field\n",
    "       log( d(k*mu)/dl ) (y) = log( sum_i k(y-x_i)*mu_i ) \n",
    "    \"\"\"\n",
    "    loglikelihoods = kernel_product_log(Nu[1], Mu[1], Mu[0].log(), mode, s)\n",
    "    dMuNu          = - torch.dot( loglikelihoods.view(-1)  , Nu[0].view(-1) )\n",
    "    \n",
    "    kernel_heatmap = None\n",
    "    if info :\n",
    "        # Create a uniform grid on the [-2,+2]x[-2,+2] square:\n",
    "        res    = 100 ; ticks = np.linspace( -2, 2, res + 1)[:-1] + 1/(2*res) \n",
    "        X,Y    = np.meshgrid( ticks, ticks )\n",
    "        points = Variable(torch.from_numpy(np.vstack( (X.ravel(), Y.ravel()) ).T).type(dtype), requires_grad=False)\n",
    "        \n",
    "        # Sample the log-likelihood on this grid:\n",
    "        kernel_heatmap   = kernel_product_log(points, Mu[1], Mu[0].log(), mode, s)\n",
    "        kernel_heatmap   = kernel_heatmap.view(res,res) # reshape as a \"background\" image\n",
    "    return dMuNu, kernel_heatmap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MyModel = GenerativeModel(degree = 2)\n",
    "# Please play around with several kernel radii and profiles!\n",
    "FitModel(MyModel, kernel_neglog_likelihood, (Nu_j, Y_j), info_type=\"log\", name=\"loglikelihood\", \n",
    "         mode = \"gaussian\", s=.25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using an Optimal Transport distance\n",
    "\n",
    "In the previous two sections, we've seen how to compute **kernel distances**, which are the duals of Sobolev-like norms on space of functionals, as well as **Maximum Likelihood scores**, which can be understood as approximations of the Kullback-Leibler divergence.\n",
    "Last but not least, we now show how to compute **Optimal Transport** plans efficiently, ending up on Wasserstein-like distances between unlabeled measures.\n",
    "\n",
    "**Getting used to Optimal Transport.** The modern OT theory relies on a few objects\n",
    "and problems that we now briefly recall. For a complete reference on the subject,\n",
    "you may find useful Filippo Santambrogio's *Optimal Transport for Applied Mathematicians* (2015)\n",
    "or PeyrÃ©-Cuturi's *Computational Optimal Transport* (2017), depending on your\n",
    "background.\n",
    "\n",
    "**Kantorovitch problem.** \n",
    "Given $\\mu = \\sum_i \\mu_i \\,\\delta_{x_i}$ and $\\nu = \\sum_j \\nu_j\\,\\delta_{y_j}$ we wish\n",
    "to find a **Transport Plan** $\\Gamma$ (a measure on the product\n",
    "$\\{x_i,\\dots\\}\\times\\{y_j,\\dots\\}$, encoded as an $N$-by-$M$ matrix $(\\Gamma(x_i\\leftrightarrow y_j))=(\\gamma_{i,j})$) which\n",
    "is a solution of the following optimization problem:\n",
    "\n",
    "$$\\text{minimize} ~~~ \\langle \\Gamma, C \\rangle ~=~ \\sum_{i,j} \\gamma_{i,j} C_{i,j}$$\n",
    "$$\\text{subject to:} ~~~~ \\forall\\,i,j,~~ \\gamma_{i,j} \\geqslant 0, \n",
    "~~ \\sum_j \\gamma_{i,j} ~=~ \\mu_i,\n",
    "~~ \\sum_i \\gamma_{i,j} ~=~ \\nu_j,$$\n",
    "\n",
    "where the **Cost matrix** $C_{i,j} = c(x_i,y_j)$ encodes\n",
    "the cost of moving a unit mass from point $x_i$ to point $y_j$.\n",
    "\n",
    "**Wasserstein distance.** If one uses $c(x_i,y_j)=\\|x_i-y_j\\|^2$,\n",
    "the optimal value of the above problem is called the **Wasserstein distance**\n",
    "$\\text{d}_{\\text{Wass}}(\\mu,\\nu)$ between measures $\\mu$ and $\\nu$.\n",
    "Its theoretical properties are plentiful... But can we compute it efficiently?\n",
    "**In the general high-dimensional case: no, we can't.**\n",
    "Indeed, the Kantorovitch problem above is a textbook **Linear optimization problem**,\n",
    "combinatorial by nature. Even though the simplex algorithm or other classical routines\n",
    "output exact solutions, they do so at a prohibitive cost: at least cubic wrt. the number of samples."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Entropic regularization.**\n",
    "Thankfully, we can however **compute approximate transport plans at a much lower cost**.\n",
    "Given a small regularization parameter $\\varepsilon$, the idea is to\n",
    "add an **entropic barrier** to the Linear Kantorovitch problem and solve\n",
    "\n",
    "$$\\text{minimize} ~~~ \\langle \\Gamma, C \\rangle - \\varepsilon \\text{H}(\\Gamma) \n",
    "~=~ \\sum_{i,j} \\gamma_{i,j} C_{i,j} + \\varepsilon\\,\\sum_{i,j} \\gamma_{i,j}\\, \\log \\gamma_{i,j}$$\n",
    "$$\\text{subject to:} ~~~~ \\forall\\,i,j,~~ \\gamma_{i,j} \\geqslant 0, \n",
    "~~ \\sum_j \\gamma_{i,j} ~=~ \\mu_i,\n",
    "~~ \\sum_i \\gamma_{i,j} ~=~ \\nu_j.$$\n",
    "\n",
    "An important property of the $x\\mapsto x\\log x$ function is that it has\n",
    "**a $-\\infty$ derivative at location $x=0$**. Since the main objective\n",
    "function $\\langle \\Gamma, C\\rangle$ is linear wrt. the $\\gamma_{i,j}$'s, this implies that\n",
    "the optimal value of the regularized problem is attained **in the relative interior\n",
    "of the simplex, defined by the constraints:**\n",
    "\n",
    "$$ \\Gamma ~>~0, ~~~ \\Gamma 1 ~=~ \\mu, ~~~ \\Gamma^T 1 ~=~\\nu.$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hence, **the optimum is necessarily reached at a critical point of our constrained problem**.\n",
    "At the optimum $\\Gamma^{\\star}$, the gradient of the objective can thus be written as a linear combination of\n",
    "the equality constraints' gradients:\n",
    "\n",
    "$$\\exists\\, (u_i)\\in\\mathbb{R}^N, ~(v_j)\\in\\mathbb{R}^M,~~ \\forall\\,i,j,~~~ C_{i,j} + \\varepsilon \\,(1+\\log \\gamma_{i,j}^{\\star})~=~ u_i + v_j $$\n",
    "\n",
    "where $u_i$ is the coefficient associated to the constraint $\\sum_j \\gamma_{i,j} = \\mu_i$, as $v_j$ is linked to $\\sum_i \\gamma_{i,j} = \\nu_j$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All in all, we see that the optimal transport plan $(\\gamma^{\\star}_{i,j}) \\in \\mathbb{R}_+^{N\\times M}$ is\n",
    "characterized by **a single pair of vectors $(u,v) \\in \\mathbb{R}^{N+M}$**:\n",
    "\n",
    "$$\\forall\\,i,j,~~~~~ 1+\\log \\gamma^{\\star}_{i,j} ~=~ (u_i+v_j-C_{i,j})/\\varepsilon$$\n",
    "\n",
    "$$\\text{i.e.}~~~ \\Gamma^{\\star}~=~ \\text{diag}(a_i)\\,K_{i,j}\\,\\text{diag}(b_j)$$\n",
    "\n",
    "$$\\text{with}~~~ a_i~=~ \\tfrac{1}{\\sqrt{e}}\\,\\exp(u_i/\\varepsilon), \n",
    "~~ b_j~=~\\tfrac{1}{\\sqrt{e}}\\,\\exp(v_j/\\varepsilon), ~~ K_{i,j}~=~\\exp(-C_{i,j}/\\varepsilon).$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consequences of this \"critical point equation\" are twofold:\n",
    "\n",
    "- The **dimension of the space in which we should search the optimal transport plan is greatly reduced**,\n",
    "  jumping from $(M\\times N)$ to $(M+N)$ - the adjoint variables associated to the equality constraints. \n",
    "  Furthermore, the optimal value of the cost can be computed using this **cheap formula**:\n",
    "  \n",
    "  $$\\langle \\Gamma^{\\star}, C\\rangle - \\varepsilon \\text{H}(\\Gamma^{\\star}) \n",
    "  ~=~ \\sum_{i,j} \\gamma^{\\star}_{i,j} \\,\\big( C_{i,j} + \\varepsilon\\,\\log \\gamma^{\\star}_{i,j} \\big)\n",
    "  ~=~ \\sum_{i,j} \\gamma_{i,j} \\,(u_i + v_j - \\varepsilon) $$\n",
    "  \n",
    "  $$\\text{i.e.}~~~ \\langle \\Gamma^{\\star}, C\\rangle - \\varepsilon \\text{H}(\\Gamma^{\\star}) \n",
    "  ~=~ \\langle \\mu, u \\rangle + \\langle \\nu, v\\rangle- \\varepsilon\n",
    "  ~=~ \\varepsilon\\,\\big[ \\langle \\mu, \\log(a) \\rangle + \\langle \\nu, \\log(b)\\rangle\\big].$$\n",
    "  \n",
    "- The optimal transport plan can be expressed as **the positive scaling of a positive kernel matrix $K$**. But in the meantime, it should also satisfy the two marginal constraints which can be written in terms of $(a_i)$ and $(b_j)$:\n",
    "    \n",
    "$$ a_i ~=~ \\frac{\\mu_i}{(Kb)_i}, ~~~~ b_j ~=~ \\frac{\\nu_j}{(K^Ta)_j}.$$\n",
    "\n",
    "As was first remarked by Marco Cuturi in his 2013 paper, *Sinkhorn Distances: Lightspeed Computation of Optimal Transportation Distances*,\n",
    "this reformulation of (entropic regularized) Optimal Transport can be linked to the **Sinkhorn Theorem**: It admits a unique solution $(a,b) \\in \\mathbb{R}^{N+M}$, which can be approached iteratively by applying the steps\n",
    "\n",
    "$$ a^{(0)} = (1,\\dots, 1),~ b^{(0)} = (1,\\dots, 1), ~~~~~~ a^{(n+1)} ~=~ \\frac{\\mu}{Kb^{(n)}}, ~~~~ b^{(n+1)} ~=~ \\frac{\\nu}{K^Ta^{(n+1)}}.$$\n",
    "\n",
    "Hence, **one can solve the regularized Optimal Transport problem by iterating kernel products (aka. discrete convolutions) and pointwise divisions, on variables which have the same memory footprint as the input measures!** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [],
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def wasserstein_distance(Mu, Nu, info = False, mode = \"gaussian\", s = .2) :\n",
    "    \"\"\"\n",
    "    Executes the Sinkhorn algorithm to compute an approximate Optimal Transport\n",
    "    cost between Mu and Nu, using an entropic regularization of strength\n",
    "    epsilon = s^2.\n",
    "    \"\"\"\n",
    "    eps = s**2\n",
    "    A   = Variable(torch.ones(Mu[0].size()).type(dtype))\n",
    "    B   = Variable(torch.ones(Nu[0].size()).type(dtype))\n",
    "    for it in range(1000) : # Sinkhorn loop ---------------------------------------\n",
    "        A_prev = A\n",
    "        A = Mu[0] / kernel_product(Mu[1], Nu[1], B,mode,s).view(-1)\n",
    "        B = Nu[0] / kernel_product(Nu[1], Mu[1], A,mode,s).view(-1)\n",
    "        err = (A.log()-A_prev.log()).abs().mean().data.cpu().numpy()\n",
    "        if err < 1e-5 :\n",
    "            print(it, \": \", err, \", \", end=\"\")\n",
    "            break\n",
    "    \n",
    "    # At convergence, one can compute the cost using  -----------------------------   \n",
    "    # the \"dual\", cheap formula in terms of the adjoint variables:\n",
    "    D2_d = eps * ( torch.dot(Mu[0].view(-1), A.log().view(-1)) \\\n",
    "                 + torch.dot(Nu[0].view(-1), B.log().view(-1)) )\n",
    "        \n",
    "    transport_plan = None\n",
    "    if info : # If required, compute the full \\Gamma^{\\star}. This is expensive to plot!\n",
    "        x_i = Mu[1].unsqueeze(1) ; y_j = Nu[1].unsqueeze(0)\n",
    "        xmy = ((x_i-y_j)**2).sum(2)\n",
    "        if mode == \"gaussian\" : K = torch.exp( - xmy / (s**2) )\n",
    "        else : raise NotImplementedError(\"Please implement your custom distance+kernel !\")\n",
    "            \n",
    "        transport_plan = K * (A.view(-1,1) * B.view(1,-1))\n",
    "    return D2_d, transport_plan"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**N.B.:** In the Sinkhorn loop, you may wish to **compute and store the Kernel matrix $K(x_i,y_j)$ in advance**. This will greatly reduce the computation time of the Sinkhorn-Wasserstein cost. Note, however, that this is only tractable for **small point clouds** ($M,N < 10,000$) as the full $M$-by-$N$ matrix may not fit in the GPU memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "MyModel = GenerativeModel(degree = 2)\n",
    "FitModel(MyModel, wasserstein_distance, (Nu_j, Y_j), \n",
    "         info_type = \"transport\", name = \"OT\", mode=\"gaussian\", s = .15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sinkhorn  algorithm in the log domain\n",
    "\n",
    "Is the scheme presented above stable enough? No, it isn't. Indeed, as discussed in the section dedicated to Maximum likelihood estimators, **if we use kernels in multiplicative formulas, we should favor log-domain implementations**. As seen below, our `kernel_product_log` operator works just fine:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def wasserstein_distance_log(Mu, Nu, info = False, s = .2, tau=-.5) :\n",
    "    \"\"\"\n",
    "    Log-domain implementation of the Sinkhorn algorithm,\n",
    "    provided for numerical stability.\n",
    "    The \"multiplicative\" standard implementation is replaced\n",
    "    by an \"additive\" logarithmic one, as:\n",
    "    - A is replaced by U_i = log(A_i) = (u_i/eps - .5)\n",
    "    - B is replaced by V_j = log(B_j) = (v_j/eps - .5)\n",
    "    - K_ij is replaced by C_ij = - eps * log(K_ij)\n",
    "                               = |X_i-Y_j|^2\n",
    "    (remember that epsilon = eps = s^2)\n",
    "    \n",
    "    The update step:\n",
    "    \n",
    "    \" a_i = mu_i / \\sum_j k(x_i,y_j) b_j \"\n",
    "    \n",
    "    is thus replaced, applying log(...) on both sides, by\n",
    "    \n",
    "    \" u_i = log(mu_i) - log(sum( exp(-C_ij/eps) * exp(V_j) )) ] \"\n",
    "    \n",
    "    N.B.: By default, we use a slight extrapolation to let the algorithm converge faster.\n",
    "          As this may let our algorithm diverge... Please set tau=0 to fall back on the\n",
    "          standard Sinkhorn algorithm.\n",
    "    \"\"\"\n",
    "    eps = s**2\n",
    "    U   = Variable(torch.zeros(Mu[0].size()).type(dtype))\n",
    "    V   = Variable(torch.zeros(Nu[0].size()).type(dtype))\n",
    "    for it in range(1000) :\n",
    "        U_prev = U\n",
    "        U = tau*U + (1-tau)*( torch.log(Mu[0]) - kernel_product_log(Mu[1], Nu[1], V, \"gaussian\", s).view(-1) ) \n",
    "        V = tau*V + (1-tau)*( torch.log(Nu[0]) - kernel_product_log(Nu[1], Mu[1], U, \"gaussian\", s).view(-1) )\n",
    "        err = (eps * (U-U_prev).abs().mean()).data.cpu().numpy()\n",
    "        if err < 1e-5 :\n",
    "            print(it, \", \", end=\"\")\n",
    "            break\n",
    "    D2 = eps * ( torch.dot(Mu[0].view(-1), U.view(-1) ) \\\n",
    "               + torch.dot(Nu[0].view(-1), V.view(-1) ) )\n",
    "    \n",
    "    transport_plan = None\n",
    "    if info :\n",
    "        C = ((Mu[1].unsqueeze(1) - Nu[1].unsqueeze(0) )**2).sum(2)\n",
    "        transport_plan = ( U.view(-1,1)+V.view(1,-1) - C/eps ).exp()\n",
    "    return D2, transport_plan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "MyModel = GenerativeModel(degree = 2)\n",
    "FitModel(MyModel, wasserstein_distance_log, (Nu_j, Y_j), info_type = \"transport\", name = \"wasserstein\", s = .15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Playing with the Sinkhorn algorithm\n",
    "\n",
    "The Sinkhorn-Wasserstein distance looks good... But it still requires about 100-500 kernel convolutions to be computed, depending on the regularization strength. Hence, couldn't we use the **intermediate results of the Sinkhorn loop** as cheap approximations to the optimal transport costs? We display below an example of \"algorithmic compromise\" which performs well in practice.\n",
    "\n",
    "Thanks to autodiff libraries, trying out new Wasserstein-like costs is easy; however, finding \"the right algorithm\" which combines efficiency with explainability is still very much a research topic! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sinkhorn_loop(Mu, Nu, s, nits, tau=-.5) :\n",
    "    eps = s**2\n",
    "    U   = Variable(torch.zeros(Mu[0].size()).type(dtype))\n",
    "    V   = Variable(torch.zeros(Nu[0].size()).type(dtype))\n",
    "    for it in range(nits) :\n",
    "        U = tau*U + (1-tau)*( torch.log(Mu[0]) - kernel_product_log(Mu[1], Nu[1], V, \"gaussian\", s).view(-1) ) \n",
    "        V = tau*V + (1-tau)*( torch.log(Nu[0]) - kernel_product_log(Nu[1], Mu[1], U, \"gaussian\", s).view(-1) )\n",
    "        \n",
    "    D2 = eps * ( torch.dot(Mu[0].view(-1), U.view(-1)) \\\n",
    "               + torch.dot(Nu[0].view(-1), V.view(-1)) )\n",
    "    \n",
    "    return D2\n",
    "\n",
    "\n",
    "def sinkhorn_distance_log(Mu, Nu, info = False, s = .2, nits = 10) :\n",
    "    \"\"\"\n",
    "    Inspired by \"Learning Generative Models with Sinkhorn Divergences\"\n",
    "    by Genevay, PeyrÃ© and Cuturi (2017, arxiv.org/abs/1706.00292), we carelessly compute\n",
    "    a loss function using only a handful of sinkhorn iterations. \n",
    "    The expression below is designed to give \"relevant\" results even\n",
    "    when s is large or when the Sinkhorn loop has not fully converged.\n",
    "    \n",
    "    This formula uses the \"dual\" Sinkhorn cost and has not been documented anywhere: \n",
    "    it is a mere experiment, a compromise between the mathematical theory of OT\n",
    "    and algorithmic efficiency. As such, it lacks a proper interpretation\n",
    "    and we prefer not to output any misleading 'informative' transport plan.\n",
    "    \"\"\"\n",
    "    \n",
    "    Loss = 2*sinkhorn_loop(Mu,Nu, s,nits) \\\n",
    "           - sinkhorn_loop(Mu,Mu, s,nits) - sinkhorn_loop(Nu,Nu, s,nits)\n",
    "    transport_plan = None\n",
    "    return Loss, transport_plan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "MyModel = GenerativeModel(degree = 2)\n",
    "FitModel(MyModel, sinkhorn_distance_log, (Nu_j, Y_j), info_type=\"transport\", name=\"sinkhorn\", s=.25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Conclusion\n",
    "\n",
    "In this notebook, we've presented three major families of \"distance\" costs between probability measures:\n",
    "- Kernel distances (also known as *Maximum Mean Discrepancies*), which descend from dual norms on **Sobolev-like spaces of functions**.\n",
    "- Empirical log-likelihoods, which converge towards **Kullback-Leibler discrepancies**.\n",
    "- Sinkhorn distances, designed as cheap approximations of Optimal Transport costs; they can be linked to dual norms on **Lipschitz-like spaces of functions**.\n",
    "\n",
    "Interestingly, the three of them use **the same atomic operation: the Kernel Product**, possibly implemented in the log-domain. As it is a GPU-friendly operation, using these formulas results in scalable algorithm.\n",
    "But which one should we use in practical applications?\n",
    "This is the main question, that can only be answered with respect to specific applications and datasets. \n",
    "\n",
    "**A rule of thumb:** kernel distances and log-likelihoods are both cheap, and differ in the way they handle **outliers** and isolated $x_i$'s or $y_j$'s. \n",
    "With kernel distances, they are nearly *forgotten* as the tail of the kernel function $k$ goes to zero;\n",
    "in the likelihood case, though, since $-\\log k(x) \\rightarrow +\\infty$ when $x$ grows, outliers tend to have a large influence on the overall cost and its gradient.\n",
    "More recently introduced, Sinkhorn distances tend to be both interpretable and robust... At a high computational cost. Computing a full transport plan to simply get a *gradient* is overkill for most applications, and future works will certainly focus on cheap approximations that interpolate between OT and simpler theories.\n",
    "\n",
    "To get your own intuition on the subject, feel free to re-run this notebook with different parameters' values!\n"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
