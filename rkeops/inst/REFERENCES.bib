@article{JMLR:v22:20-275,
  author  = {Benjamin Charlier and Jean Feydy and Joan Alexis Glaunès and François-David Collin and Ghislain Durif},
  title   = {Kernel Operations on the GPU, with Autodiff, without Memory Overflows},
  journal = {Journal of Machine Learning Research},
  year    = {2021},
  volume  = {22},
  number  = {74},
  pages   = {1-6},
  url     = {https://jmlr.org/papers/v22/20-275.html}
}

@article{fukushima_cognitron_1975,
	title = {Cognitron: {A} self-organizing multilayered neural network},
	volume = {20},
	issn = {1432-0770},
	shorttitle = {Cognitron},
	url = {https://doi.org/10.1007/BF00342633},
	doi = {10.1007/BF00342633},
	abstract = {A new hypothesis for the organization of synapses between neurons is proposed: “The synapse from neuron x to neuron y is reinforced when x fires provided that no neuron in the vicinity of y is firing stronger than y”. By introducing this hypothesis, a new algorithm with which a multilayered neural network is effectively organized can be deduced. A self-organizing multilayered neural network, which is named “cognitron”, is constructed following this algorithm, and is simulated on a digital computer. Unlike the organization of a usual brain models such as a three-layered perceptron, the self-organization of a cognitron progresses favorably without having a “teacher” which instructs in all particulars how the individual cells respond. After repetitive presentations of several stimulus patterns, the cognitron is self-organized in such a way that the receptive fields of the cells become relatively larger in a deeper layer. Each cell in the final layer integrates the information from whole parts of the first layer and selectively responds to a specific stimulus pattern or a feature.},
	language = {en},
	number = {3},
	journal = {Biological Cybernetics},
	author = {Fukushima, Kunihiko},
	month = sep,
	year = {1975},
	keywords = {Deep Layer, Final Layer, Individual Cell, Neural Network, Receptive Field},
	pages = {121--136},
}